{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the environment\n",
    "\n",
    "from hashlib import sha1\n",
    "\n",
    "\n",
    "```bash\n",
    "# In one terminal\n",
    "cd docker\n",
    "docker-compose -f docker-compose-w2.yml start\n",
    "\n",
    "# Then, in another terminal\n",
    "```bash\n",
    "cd docker-grafana\n",
    "./install-plugin.sha\n",
    "docker compose -f monitoring.yml start\n",
    "```\n",
    "Then, add port 3000 to your VS code port forwarding. \n",
    "\n",
    "Finally, open this url [http://localhost:3000/d/opensearch/opensearch-prometheus](http://localhost:3000/d/opensearch/opensearch-prometheus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['HOST'] = 'localhost'\n",
    "os.environ['BBUY_DATA'] = '/workspace/datasets/product_data/products/'\n",
    "os.environ['BBUY_QUERIES'] = '/workspace/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"bbuy_products\"}"
     ]
    }
   ],
   "source": [
    "# Index with provided field mappings\n",
    "!curl -k -X PUT -u admin:admin \"https://$HOST:9200/bbuy_products\" -H 'Content-Type: application/json' -d @/workspace/search_engineering/week1/bbuy_products.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First indexing of the BB product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Indexing /workspace/datasets/product_data/products/ to bbuy_products with 16 workers, refresh_interval of -1 to host localhost with a maximum number of docs sent per file per worker of 200000 and 500 per batch.\n",
      "INFO:Done. 1275077 were indexed in 21.870381166016646 minutes.  Total accumulated time spent in `bulk` indexing: 257.9378352573321 minutes\n"
     ]
    }
   ],
   "source": [
    "# Index with 16 worker threads and 500 batch size\n",
    "!python index.py -s $BBUY_DATA -w 16 -b 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing took 22 minutes, at about 1,000 docs per second.  \n",
    "\n",
    "![](assets/L1-first-indexing.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing the same content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index with 16 worker threads and 500 batch size (and a limit of 100,000 documents)\n",
    "!python index.py -s $BBUY_DATA -w 16 -b 500 -m 1000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing query load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 42 for worker: 0\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 84 for worker: 1\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 126 for worker: 2\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 168 for worker: 3\n",
      "^C\n",
      "\n",
      "Caught SIGINT. Shutting down workers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./query.py -q $BBUY_QUERIES/train.csv -w 4 -m 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying was at approximately 35 queries/second\n",
    "\n",
    "![](assets/L1-querying.png)\n",
    "\n",
    "The bottleneck seems to be CPU where the CPU was mostly maxed at 100%.\n",
    "\n",
    "![](assets/L1-cpu-usage.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking with double CPU (from 1 to 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Indexing /workspace/datasets/product_data/products/ to bbuy_products with 16 workers, refresh_interval of -1 to host localhost with a maximum number of docs sent per file per worker of 100 and 500 per batch.\n",
      "^C\n",
      "\n",
      "Caught SIGINT. Shutting down workers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index with 16 worker threads and 500 batch size, limiting to 1,000 documents\n",
    "!python index.py -s $BBUY_DATA -w 16 -b 500 -m 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 42 for worker: 0\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 126 for worker: 2\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 168 for worker: 3\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 84 for worker: 1\n",
      "^C\n",
      "\n",
      "Caught SIGINT. Shutting down workers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./query.py -q $BBUY_QUERIES/train.csv -w 4 -m 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on indexing\n",
    "\n",
    "Doubling CPU did not seem to have an impact on indexing which was still about 1,000 docs per second.\n",
    "\n",
    "![](assets/L2-cpu2-indexing.png)\n",
    "\n",
    "Digging deeper on the CPU impact during indexing, we see that CPU usage was not maxed, but memory usage was maxed at 2GB. Thus, during indexing, the bottleneck was memory.\n",
    "\n",
    "![](assets/L2-cpu2-indexing-cpu-use.png) ![](assets/L2-cpu2-indexing-mem-use.png)\n",
    "\n",
    "### Impact on querying.\n",
    "On the other hand, doubling CPU double queries to 70-80 queries/second. Thus, CPU is the main bottleneck for querying.\n",
    "\n",
    "![](assets/L2-cpu2-querying.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking with double RAM (from 2GB to 4GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Indexing /workspace/datasets/product_data/products/ to bbuy_products with 16 workers, refresh_interval of -1 to host localhost with a maximum number of docs sent per file per worker of 1000 and 500 per batch.\n",
      "INFO:Done. 255077 were indexed in 7.939172314800089 minutes.  Total accumulated time spent in `bulk` indexing: 89.46414424226775 minutes\n"
     ]
    }
   ],
   "source": [
    "# Index with 16 worker threads and 500 batch size, limiting to 1,000 documents\n",
    "!python index.py -s $BBUY_DATA -w 16 -b 500 -m 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 42 for worker: 0\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 84 for worker: 1\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 126 for worker: 2\n",
      "INFO:Loading query file from /workspace/datasets//train.csv and using seed 168 for worker: 3\n",
      "^C\n",
      "\n",
      "Caught SIGINT. Shutting down workers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./query.py -q $BBUY_QUERIES/train.csv -w 4 -m 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on indexing\n",
    "\n",
    "Doubling memory did not seem to increase indexin rate. Interestingly, it seemed to drop to 600 docs per second.\n",
    "\n",
    "![](assets/L2-mem4-indexing.png)\n",
    "\n",
    "Not sure what was causing this, because CPU and memory usage did not seem saturated.\n",
    "\n",
    "![](assets/L2-mem4-indexing-cpu-mem-use.png)\n",
    "\n",
    "### Impact on querying.\n",
    "Doubling memory had no impact on querying rate.\n",
    "\n",
    "![](assets/L2-mem4-querying.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
